{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6ee839-629e-4394-9ad5-a994dbaf727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI \n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.document_loaders import  PyPDFLoader\n",
    "from langchain.vectorstores import  FAISS\n",
    "from langchain.text_splitter import  RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings \n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "from time import monotonic\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from typing_extensions import TypedDict\n",
    "from IPython.display import display, Image\n",
    "from typing import List, TypedDict\n",
    "\n",
    "# from ragas import evaluate\n",
    "# from ragas.metrics import (\n",
    "#     answer_correctness,\n",
    "#     faithfulness,\n",
    "#     answer_relevancy,\n",
    "#     context_recall,\n",
    "#     answer_similarity\n",
    "# )\n",
    "\n",
    "import langgraph\n",
    "\n",
    "\n",
    "### Helper functions for the notebook\n",
    "from helper_functions import num_tokens_from_string, replace_t_with_space, replace_double_lines_with_one_line, split_into_chapters,\\\n",
    "analyse_metric_results, escape_quotes, text_wrap,extract_book_quotes_as_documents\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "os.environ[\"PYDEVD_WARN_EVALUATION_TIMEOUT\"] = \"100000\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa956bb-f7c1-4d42-922c-f99045d513b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_frames(video_file, num_frames):\n",
    "    video = cv2.VideoCapture(video_file)\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    interval = max(total_frames // num_frames, 1)\n",
    "    frames = []\n",
    "    for i in range(total_frames):\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "        if i % interval == 0:\n",
    "            pil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            frames.append(pil_img)\n",
    "    video.release()\n",
    "    return frames\n",
    "\n",
    "def multimodal_predict(message):\n",
    "    txt = message['text']\n",
    "    if message['files']:\n",
    "        if len(message['files']) == 1:\n",
    "            image = [message['files'][-1]['path']]\n",
    "        elif len(message['files']) > 1:\n",
    "            image = [msg['path'] for msg in message['files']]\n",
    "    else:\n",
    "        raise ValueError(\"You need to upload an image or video for LLaVA to work.\")\n",
    "\n",
    "    video_extensions = (\"avi\", \"mp4\", \"mov\", \"mkv\", \"flv\", \"wmv\", \"mjpeg\")\n",
    "    image_extensions = tuple([ex for ex, _ in Image.registered_extensions().items()])\n",
    "\n",
    "    if len(image) == 1:\n",
    "        if image[0].endswith(video_extensions):\n",
    "            image = sample_frames(image[0], 12)\n",
    "            image_tokens = \"<image>\" * 13\n",
    "            prompt = f\"<|im_start|>user {image_tokens}\\n{message['text']}<|im_end|><|im_start|>assistant\"\n",
    "        elif image[0].endswith(image_extensions):\n",
    "            image = Image.open(image[0]).convert(\"RGB\")\n",
    "            prompt = f\"<|im_start|>user <image> The response to the query must be in very great detail \\n{message['text']}<|im_end|><|im_start|>assistant\"\n",
    "    elif len(image) > 1:\n",
    "        image_list = []\n",
    "        for img in image:\n",
    "            if img.endswith(image_extensions):\n",
    "                img = Image.open(img).convert(\"RGB\")\n",
    "                image_list.append(img)\n",
    "            elif img.endswith(video_extensions):\n",
    "                frames = sample_frames(img, 6)\n",
    "                image_list.extend(frames)\n",
    "\n",
    "        toks = \"<image>\" * len(image_list)\n",
    "        prompt = f\"<|im_start|>user {toks}\\n{message['text']}<|im_end|><|im_start|>assistant\"\n",
    "        image = image_list\n",
    "\n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
    "    output = multimodal_model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
    "    generated_output = processor.decode(output[0][2:], skip_special_tokens=True)\n",
    "    return generated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520dd52f-ab60-4307-a4d4-397dacf29e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "groq_api_key = \"\"\n",
    "\n",
    "guides_path='IICRC.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47c461e-3baa-4779-aac3-b4f11ebb6da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = split_into_chapters(guides_path, start_page=89)\n",
    "chapters = replace_t_with_space(chapters)\n",
    "print(len(chapters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d03719-01f8-4a03-b7be-cdc2e760b8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_prompt_template = \"\"\"Write an extensive summary of the following, maintain headings as they're important:\n",
    "\n",
    "{text}\n",
    "\n",
    "SUMMARY:\"\"\"\n",
    "\n",
    "summarization_prompt = PromptTemplate(template=summarization_prompt_template, input_variables=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22ef82c-2ddd-4785-a87c-5a217a9c379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chapter_summary(chapter):\n",
    "    \"\"\"\n",
    "    Creates a summary of a chapter using a large language model (LLM).\n",
    "\n",
    "    Args:\n",
    "        chapter: A Document object representing the chapter to summarize.\n",
    "\n",
    "    Returns:\n",
    "        A Document object containing the summary of the chapter.\n",
    "    \"\"\"\n",
    "\n",
    "    chapter_txt = chapter.page_content  # Extract chapter text\n",
    "    model_name = \"gpt-4o\"  # Specify LLM model\n",
    "    llm = ChatOpenAI(temperature=0, model_name=model_name)  # Create LLM instance\n",
    "    gpt_35_turbo_max_tokens = 16000  # Maximum token limit for the LLM\n",
    "    verbose = False  # Set to True for more detailed output\n",
    "\n",
    "    # Calculate number of tokens in the chapter text\n",
    "    num_tokens = num_tokens_from_string(chapter_txt, model_name)\n",
    "\n",
    "    # Choose appropriate chain type based on token count\n",
    "    if num_tokens < gpt_35_turbo_max_tokens:\n",
    "        chain = load_summarize_chain(llm, chain_type=\"stuff\", prompt=summarization_prompt, verbose=verbose) \n",
    "    else:\n",
    "        chain = load_summarize_chain(llm, chain_type=\"map_reduce\", map_prompt=summarization_prompt, combine_prompt=summarization_prompt, verbose=verbose)\n",
    "\n",
    "    start_time = monotonic()  # Start timer\n",
    "    doc_chapter = Document(page_content=chapter_txt)  # Create Document object for chapter\n",
    "    summary = chain.invoke([doc_chapter])  # Generate summary using the chain\n",
    "    print(f\"Chain type: {chain.__class__.__name__}\")  # Print chain type\n",
    "    print(f\"Run time: {monotonic() - start_time}\")  # Print execution time\n",
    "\n",
    "    # summary = replace_double_lines_with_one_line(summary[\"output_text\"])\n",
    "\n",
    "    doc_summary = Document(page_content=summary, metadata=chapter.metadata)\n",
    "\n",
    "    return doc_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c04cb2-131a-4913-a9b5-7771a1c3272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preliminary_inspection_chapter = chapters[9]\n",
    "preliminary_inspection_summary = create_chapter_summary(preliminary_inspection_chapter)\n",
    "preliminary_inspection_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f4a773-00d7-42e5-88a2-f1c15832abc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_book(path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Encodes a PDF book into a vector store using OpenAI embeddings.\n",
    "\n",
    "    Args:\n",
    "        path: The path to the PDF file.\n",
    "        chunk_size: The desired size of each text chunk.\n",
    "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        A FAISS vector store containing the encoded book content.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load PDF documents\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    cleaned_texts = replace_t_with_space(texts)\n",
    "\n",
    "    # Create embeddings and vector store\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(cleaned_texts, embeddings)\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00dc554-7b49-4c46-bcbf-127a81a03413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### IF VECTOR STORES ALREADY EXIST, LOAD THEM\n",
    "if os.path.exists(\"chunks_vector_store\"):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    chunks_vector_store =  FAISS.load_local(\"chunks_vector_store\", embeddings, allow_dangerous_deserialization=True)\n",
    "else:\n",
    "    chunks_vector_store = encode_book(guides_path, chunk_size=1000, chunk_overlap=200)\n",
    "    chunks_vector_store.save_local(\"chunks_vector_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8905d54-4d45-4a82-b902-3245f1891660",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_query_retriever = chunks_vector_store.as_retriever(search_kwargs={\"k\": 1})   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccb0751-abbb-484a-a740-83da52d3b1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context_per_question(state):\n",
    "    \"\"\"\n",
    "    Retrieves relevant context for a given question. The context is retrieved from the book chunks and chapter summaries.\n",
    "\n",
    "    Args:\n",
    "        state: A dictionary containing the question to answer.\n",
    "    \"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    print(\"Retrieving relevant chunks...\")\n",
    "    question = state[\"question\"]\n",
    "    docs = chunks_query_retriever.get_relevant_documents(question)\n",
    "\n",
    "    # Concatenate document content\n",
    "    context = \" \".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "\n",
    "    # print(\"Retrieving relevant chapter summaries...\")\n",
    "    # docs_summaries = chapter_summaries_query_retriever.get_relevant_documents(state[\"question\"])\n",
    "\n",
    "    # # Concatenate chapter summaries with citation information\n",
    "    # context_summaries = \" \".join(\n",
    "    #     f\"{doc.page_content} (Chapter {doc.metadata['chapter']})\" for doc in docs_summaries\n",
    "    # )\n",
    "\n",
    "    # print(\"Retrieving relevant book quotes...\")\n",
    "    # docs_book_quotes = book_quotes_query_retriever.get_relevant_documents(state[\"question\"])\n",
    "    # book_qoutes = \" \".join(doc.page_content for doc in docs_book_quotes)\n",
    "\n",
    "\n",
    "    # all_contexts = context + context_summaries + book_qoutes\n",
    "    # all_contexts = escape_quotes(all_contexts)\n",
    "\n",
    "    return {\"context\": context, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38032aa8-cf7d-4a75-8989-3b795afaff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_only_relevant_content_prompt_template = \"\"\"you receive a query: {query} and retrieved documents: {retrieved_documents} from a\n",
    " vector store.\n",
    " You need to filter out all the non relevant information that don't supply important information regarding the {query}.\n",
    " your goal is just to filter out the non relevant information.\n",
    " you can remove parts of sentences that are not relevant to the query or remove whole sentences that are not relevant to the query.\n",
    " DO NOT ADD ANY NEW INFORMATION THAT IS NOT IN THE RETRIEVED DOCUMENTS.\n",
    " output the filtered relevant content.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class KeepRelevantContent(BaseModel):\n",
    "    relevant_content: str = Field(description=\"The relevant content from the retrieved documents that is relevant to the query.\")\n",
    "\n",
    "keep_only_relevant_content_prompt = PromptTemplate(\n",
    "    template=keep_only_relevant_content_prompt_template,\n",
    "    input_variables=[\"query\", \"retrieved_documents\"],\n",
    ")\n",
    "\n",
    "\n",
    "keep_only_relevant_content_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=2000)\n",
    "keep_only_relevant_content_chain = keep_only_relevant_content_prompt | keep_only_relevant_content_llm.with_structured_output(KeepRelevantContent)\n",
    "\n",
    "\n",
    "def keep_only_relevant_content(state):\n",
    "    \"\"\"\n",
    "    Keeps only the relevant content from the retrieved documents that is relevant to the query.\n",
    "\n",
    "    Args:\n",
    "        question: The query question.\n",
    "        context: The retrieved documents.\n",
    "        chain: The LLMChain instance.\n",
    "\n",
    "    Returns:\n",
    "        The relevant content from the retrieved documents that is relevant to the query.\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    input_data = {\n",
    "    \"query\": question,\n",
    "    \"retrieved_documents\": context\n",
    "}\n",
    "    print(\"keeping only the relevant content...\")\n",
    "    pprint(\"--------------------\")\n",
    "    output = keep_only_relevant_content_chain.invoke(input_data)\n",
    "    relevant_content = output.relevant_content\n",
    "    relevant_content = \"\".join(relevant_content)\n",
    "    relevant_content = escape_quotes(relevant_content)\n",
    "\n",
    "    return {\"relevant_context\": relevant_content, \"context\": context, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e663f3-b1c6-4fdd-ae4c-8d9154dfdd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewriteQuestion(BaseModel):\n",
    "    \"\"\"\n",
    "    Output schema for the rewritten question.\n",
    "    \"\"\"\n",
    "    rewritten_question: str = Field(description=\"The improved question optimized for vectorstore retrieval.\")\n",
    "    explanation: str = Field(description=\"The explanation of the rewritten question.\")\n",
    "\n",
    "rewrite_question_string_parser = JsonOutputParser(pydantic_object=RewriteQuestion)\n",
    "\n",
    "\n",
    "rewrite_llm = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\", groq_api_key=groq_api_key, max_tokens=4000)\n",
    "rewrite_prompt_template = \"\"\"You are a question re-writer that converts an input question to a better version optimized for vectorstore retrieval.\n",
    " Analyze the input question {question} and try to reason about the underlying semantic intent / meaning.\n",
    " {format_instructions}\n",
    " \"\"\"\n",
    "\n",
    "rewrite_prompt = PromptTemplate(\n",
    "    template=rewrite_prompt_template,\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": rewrite_question_string_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "question_rewriter = rewrite_prompt | rewrite_llm | rewrite_question_string_parser  # Combine prompt, LLM, and parser\n",
    "\n",
    "def rewrite_question(state):\n",
    "    \"\"\"Rewrites the given question using the LLM.\n",
    "\n",
    "    Args:\n",
    "        state: A dictionary containing the question to rewrite.\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    print(\"Rewriting the question...\")\n",
    "    result = question_rewriter.invoke({\"question\": question})\n",
    "    new_question = result[\"rewritten_question\"]\n",
    "    return {\"question\": new_question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5120d5f1-84d8-4c74-928e-4592560011af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionAnswerFromContext(BaseModel):\n",
    "    answer_based_on_content: str = Field(description=\"generates an answer to a query based on a given context.\")\n",
    "\n",
    "question_answer_from_context_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=2000)\n",
    "\n",
    "\n",
    "question_answer_cot_prompt_template = \"\"\" \n",
    "Examples of Chain-of-Thought Reasoning\n",
    "\n",
    "Example 1\n",
    "\n",
    "Context: Mary is taller than Jane. Jane is shorter than Tom. Tom is the same height as David.\n",
    "Question: Who is the tallest person?\n",
    "Reasoning Chain:\n",
    "The context tells us Mary is taller than Jane\n",
    "It also says Jane is shorter than Tom\n",
    "And Tom is the same height as David\n",
    "So the order from tallest to shortest is: Mary, Tom/David, Jane\n",
    "Therefore, Mary must be the tallest person\n",
    "\n",
    "Example 2\n",
    "Context: Harry was reading a book about magic spells. One spell allowed the caster to turn a person into an animal for a short time. Another spell could levitate objects.\n",
    " A third spell created a bright light at the end of the caster's wand.\n",
    "Question: Based on the context, if Harry cast these spells, what could he do?\n",
    "Reasoning Chain:\n",
    "The context describes three different magic spells\n",
    "The first spell allows turning a person into an animal temporarily\n",
    "The second spell can levitate or float objects\n",
    "The third spell creates a bright light\n",
    "If Harry cast these spells, he could turn someone into an animal for a while, make objects float, and create a bright light source\n",
    "So based on the context, if Harry cast these spells he could transform people, levitate things, and illuminate an area\n",
    "Instructions.\n",
    "\n",
    "Example 3 \n",
    "Context: Harry Potter woke up on his birthday to find a present at the end of his bed. He excitedly opened it to reveal a Nimbus 2000 broomstick.\n",
    "Question: Why did Harry receive a broomstick for his birthday?\n",
    "Reasoning Chain:\n",
    "The context states that Harry Potter woke up on his birthday and received a present - a Nimbus 2000 broomstick.\n",
    "However, the context does not provide any information about why he received that specific present or who gave it to him.\n",
    "There are no details about Harry's interests, hobbies, or the person who gifted him the broomstick.\n",
    "Without any additional context about Harry's background or the gift-giver's motivations, there is no way to determine the reason he received a broomstick as a birthday present.\n",
    "\n",
    "For the question below, provide your answer by first showing your step-by-step reasoning process, breaking down the problem into a chain of thought before arriving at the final answer,\n",
    " just like in the previous examples.\n",
    "Context\n",
    "{context}\n",
    "Question\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "question_answer_from_context_cot_prompt = PromptTemplate(\n",
    "    template=question_answer_cot_prompt_template,\n",
    "    input_variables=[\"context\", \"question\"],\n",
    ")\n",
    "question_answer_from_context_cot_chain = question_answer_from_context_cot_prompt | question_answer_from_context_llm.with_structured_output(QuestionAnswerFromContext)\n",
    "\n",
    "\n",
    "def answer_question_from_context(state):\n",
    "    \"\"\"\n",
    "    Answers a question from a given context.\n",
    "\n",
    "    Args:\n",
    "        question: The query question.\n",
    "        context: The context to answer the question from.\n",
    "        chain: The LLMChain instance.\n",
    "\n",
    "    Returns:\n",
    "        The answer to the question from the context.\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    context = state[\"aggregated_context\"] if \"aggregated_context\" in state else state[\"context\"]\n",
    "\n",
    "    input_data = {\n",
    "    \"question\": question,\n",
    "    \"context\": context\n",
    "}\n",
    "    print(\"Answering the question from the retrieved context...\")\n",
    "\n",
    "    output = question_answer_from_context_cot_chain.invoke(input_data)\n",
    "    answer = output.answer_based_on_content\n",
    "    print(f'answer before checking hallucination: {answer}')\n",
    "    return {\"answer\": answer, \"context\": context, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c0cc64-76c8-4ace-bf2e-924d5c0d77c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_relevant_content_prompt_template = \"\"\"you receive a query: {query} and a context: {context} retrieved from a vector store. \n",
    "You need to determine if the document is relevant to the query. \n",
    "\n",
    "{format_instructions}\"\"\"\n",
    "\n",
    "class Relevance(BaseModel):\n",
    "    is_relevant: bool = Field(description=\"Whether the document is relevant to the query.\")\n",
    "    explanation: str = Field(description=\"An explanation of why the document is relevant or not.\")\n",
    "\n",
    "is_relevant_json_parser = JsonOutputParser(pydantic_object=Relevance)\n",
    "is_relevant_llm = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\", groq_api_key=groq_api_key, max_tokens=4000)\n",
    "\n",
    "is_relevant_content_prompt = PromptTemplate(\n",
    "    template=is_relevant_content_prompt_template,\n",
    "    input_variables=[\"query\", \"context\"],\n",
    "    partial_variables={\"format_instructions\": is_relevant_json_parser.get_format_instructions()},\n",
    ")\n",
    "is_relevant_content_chain = is_relevant_content_prompt | is_relevant_llm | is_relevant_json_parser\n",
    "\n",
    "def is_relevant_content(state):\n",
    "    \"\"\"\n",
    "    Determines if the document is relevant to the query.\n",
    "\n",
    "    Args:\n",
    "        question: The query question.\n",
    "        context: The context to determine relevance.\n",
    "    \"\"\"\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    input_data = {\n",
    "    \"query\": question,\n",
    "    \"context\": context\n",
    "}\n",
    "\n",
    "    # Invoke the chain to determine if the document is relevant\n",
    "    output = is_relevant_content_chain.invoke(input_data)\n",
    "    print(\"Determining if the document is relevant...\")\n",
    "    if output[\"is_relevant\"] == True:\n",
    "        print(\"The document is relevant.\")\n",
    "        return \"relevant\"\n",
    "    else:\n",
    "        print(\"The document is not relevant.\")\n",
    "        return \"not relevant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feab5308-6fec-44b8-a046-d05f40061c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class is_grounded_on_facts(BaseModel):\n",
    "    \"\"\"\n",
    "    Output schema for the rewritten question.\n",
    "    \"\"\"\n",
    "    grounded_on_facts: bool = Field(description=\"Answer is grounded in the facts, 'yes' or 'no'\")\n",
    "\n",
    "is_grounded_on_facts_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=2000)\n",
    "is_grounded_on_facts_prompt_template = \"\"\"You are a fact-checker that determines if the given answer {answer} is grounded in the given context {context}\n",
    "you don't mind if it doesn't make sense, as long as it is grounded in the context.\n",
    "output a json containing the answer to the question, and appart from the json format don't output any additional text.\n",
    "\n",
    " \"\"\"\n",
    "is_grounded_on_facts_prompt = PromptTemplate(\n",
    "    template=is_grounded_on_facts_prompt_template,\n",
    "    input_variables=[\"context\", \"answer\"],\n",
    ")\n",
    "is_grounded_on_facts_chain = is_grounded_on_facts_prompt | is_grounded_on_facts_llm.with_structured_output(is_grounded_on_facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bd59f4-6ff5-4777-b458-d207e4fcabcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "can_be_answered_prompt_template = \"\"\"You receive a query: {question} and a context: {context}. \n",
    "You need to determine if the question can be fully answered based on the context.\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "class QuestionAnswer(BaseModel):\n",
    "    can_be_answered: bool = Field(description=\"binary result of whether the question can be fully answered or not\")\n",
    "    explanation: str = Field(description=\"An explanation of why the question can be fully answered or not.\")\n",
    "\n",
    "can_be_answered_json_parser = JsonOutputParser(pydantic_object=QuestionAnswer)\n",
    "\n",
    "answer_question_prompt = PromptTemplate(\n",
    "    template=can_be_answered_prompt_template,\n",
    "    input_variables=[\"question\",\"context\"],\n",
    "    partial_variables={\"format_instructions\": can_be_answered_json_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "can_be_answered_llm = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\", groq_api_key=groq_api_key, max_tokens=4000)\n",
    "can_be_answered_chain = answer_question_prompt | can_be_answered_llm | can_be_answered_json_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a7ee1b-b9d0-478c-8eae-af0cd4960dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Grades the generation of the answer to the question based on the context, if it is grounded in the facts, and if the question can be fully answered\n",
    "\n",
    "    Args:\n",
    "        state: A dictionary containing the context, question, and answer.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Checking if the answer is grounded in the facts...\")\n",
    "    context = state[\"context\"]\n",
    "    answer = state[\"answer\"]\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    result = is_grounded_on_facts_chain.invoke({\"context\": context, \"answer\": answer})\n",
    "    grounded_on_facts = result.grounded_on_facts\n",
    "    if not grounded_on_facts:\n",
    "        print(\"The answer is hallucination.\")\n",
    "        return \"hallucination\"\n",
    "    else:\n",
    "        print(\"The answer is grounded in the facts.\")\n",
    "\n",
    "        input_data = {\n",
    "            \"question\": question,\n",
    "            \"context\": context\n",
    "        }\n",
    "\n",
    "        # Invoke the chain to determine if the question can be answered\n",
    "        print(\"Determining if the question is fully answered...\")\n",
    "        output = can_be_answered_chain.invoke(input_data)\n",
    "        can_be_answered = output[\"can_be_answered\"]\n",
    "        if can_be_answered == True:\n",
    "            print(\"The question can be fully answered.\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"The question cannot be fully answered.\")\n",
    "            return \"not_useful\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663a7505-ac39-43b4-8aaa-67d6a3d410ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = {\"question\": \"who is fluffy?\"}  # The question to answer\n",
    "context_state = retrieve_context_per_question(init_state)  # Retrieve context for the question\n",
    "relevant_content_state = keep_only_relevant_content(context_state)  # Keep only the relevant content\n",
    "is_relevant_content_state = is_relevant_content(relevant_content_state) # Check if the content is relevant\n",
    "answer_state = answer_question_from_context(relevant_content_state) # Answer the question from the context\n",
    "final_answer = grade_generation_v_documents_and_question(answer_state) # Grade the answer\n",
    "print(answer_state[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9a38ac-1aad-4c73-b640-c74848431d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualitativeRetievalAnswerGraphState(TypedDict):\n",
    "\n",
    "    question: str\n",
    "    context: str\n",
    "    answer: str\n",
    "\n",
    "qualitative_retrieval_answer_workflow = StateGraph(QualitativeRetievalAnswerGraphState)\n",
    "\n",
    "# Define the nodes\n",
    "\n",
    "# Add the nodes to the graph\n",
    "qualitative_retrieval_answer_workflow.add_node(\"retrieve_context_per_question\",retrieve_context_per_question)\n",
    "qualitative_retrieval_answer_workflow.add_node(\"keep_only_relevant_content\",keep_only_relevant_content)\n",
    "qualitative_retrieval_answer_workflow.add_node(\"rewrite_question\",rewrite_question)\n",
    "qualitative_retrieval_answer_workflow.add_node(\"answer_question_from_context\",answer_question_from_context)\n",
    "\n",
    "# Build the graph\n",
    "qualitative_retrieval_answer_workflow.set_entry_point(\"retrieve_context_per_question\")\n",
    "qualitative_retrieval_answer_workflow.add_edge(\"retrieve_context_per_question\", \"keep_only_relevant_content\")\n",
    "qualitative_retrieval_answer_workflow.add_conditional_edges(\n",
    "    \"keep_only_relevant_content\",\n",
    "    is_relevant_content,\n",
    "    {\"relevant\":\"answer_question_from_context\",\n",
    "      \"not relevant\":\"rewrite_question\"},\n",
    "    )\n",
    "qualitative_retrieval_answer_workflow.add_edge(\"rewrite_question\", \"retrieve_context_per_question\")\n",
    "qualitative_retrieval_answer_workflow.add_conditional_edges(\n",
    "\"answer_question_from_context\",\n",
    "grade_generation_v_documents_and_question,\n",
    "{\"hallucination\":\"answer_question_from_context\",\n",
    "\"not_useful\":\"rewrite_question\",\n",
    "\"useful\":END},\n",
    ")\n",
    "\n",
    "qualitative_retrieval_answer_retrival_app = qualitative_retrieval_answer_workflow.compile()\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        qualitative_retrieval_answer_retrival_app.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd16a0-18fd-4126-887a-5340f8590558",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_distilled_content_grounded_on_content_prompt_template = \"\"\"you receive some distilled content: {distilled_content} and the original context: {original_context}.\n",
    "    you need to determine if the distilled content is grounded on the original context.\n",
    "    if the distilled content is grounded on the original context, set the grounded field to true.\n",
    "    if the distilled content is not grounded on the original context, set the grounded field to false. {format_instructions}\"\"\"\n",
    "  \n",
    "\n",
    "class IsDistilledContentGroundedOnContent(BaseModel):\n",
    "    grounded: bool = Field(description=\"Whether the distilled content is grounded on the original context.\")\n",
    "    explanation: str = Field(description=\"An explanation of why the distilled content is or is not grounded on the original context.\")\n",
    "\n",
    "is_distilled_content_grounded_on_content_json_parser = JsonOutputParser(pydantic_object=IsDistilledContentGroundedOnContent)\n",
    "\n",
    "is_distilled_content_grounded_on_content_prompt = PromptTemplate(\n",
    "    template=is_distilled_content_grounded_on_content_prompt_template,\n",
    "    input_variables=[\"distilled_content\", \"original_context\"],\n",
    "    partial_variables={\"format_instructions\": is_distilled_content_grounded_on_content_json_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "is_distilled_content_grounded_on_content_llm = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\", groq_api_key=groq_api_key, max_tokens=4000)\n",
    "\n",
    "is_distilled_content_grounded_on_content_chain = is_distilled_content_grounded_on_content_prompt | is_distilled_content_grounded_on_content_llm | is_distilled_content_grounded_on_content_json_parser\n",
    "\n",
    "\n",
    "def is_distilled_content_grounded_on_content(state):\n",
    "    pprint(\"--------------------\")\n",
    "\n",
    "    \"\"\"\n",
    "    Determines if the distilled content is grounded on the original context.\n",
    "\n",
    "    Args:\n",
    "        distilled_content: The distilled content.\n",
    "        original_context: The original context.\n",
    "\n",
    "    Returns:\n",
    "        Whether the distilled content is grounded on the original context.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Determining if the distilled content is grounded on the original context...\")\n",
    "    distilled_content = state[\"relevant_context\"]\n",
    "    original_context = state[\"context\"]\n",
    "\n",
    "    input_data = {\n",
    "        \"distilled_content\": distilled_content,\n",
    "        \"original_context\": original_context\n",
    "    }\n",
    "\n",
    "    output = is_distilled_content_grounded_on_content_chain.invoke(input_data)\n",
    "    grounded = output[\"grounded\"]\n",
    "\n",
    "    if grounded:\n",
    "        print(\"The distilled content is grounded on the original context.\")\n",
    "        return \"grounded on the original context\"\n",
    "    else:\n",
    "        print(\"The distilled content is not grounded on the original context.\")\n",
    "        return \"not grounded on the original context\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c82a837-434b-4ba3-8610-bcb1d2063780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_chunks_context_per_question(state):\n",
    "    \"\"\"\n",
    "    Retrieves relevant context for a given question. The context is retrieved from the book chunks and chapter summaries.\n",
    "\n",
    "    Args:\n",
    "        state: A dictionary containing the question to answer.\n",
    "    \"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    print(\"Retrieving relevant chunks...\")\n",
    "    question = state[\"question\"]\n",
    "    docs = chunks_query_retriever.get_relevant_documents(question)\n",
    "\n",
    "    # Concatenate document content\n",
    "    context = \" \".join(doc.page_content for doc in docs)\n",
    "    context = escape_quotes(context)\n",
    "    return {\"context\": context, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b28f46-1fac-433a-bf34-edc20bbf4b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualitativeRetrievalGraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    context: str\n",
    "    relevant_context: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572b92c7-9bdc-4b43-a4ad-1b1a9b2f61bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "qualitative_chunks_retrieval_workflow = StateGraph(QualitativeRetrievalGraphState)\n",
    "\n",
    "# Define the nodes\n",
    "qualitative_chunks_retrieval_workflow.add_node(\"retrieve_chunks_context_per_question\",retrieve_chunks_context_per_question)\n",
    "qualitative_chunks_retrieval_workflow.add_node(\"keep_only_relevant_content\",keep_only_relevant_content)\n",
    "\n",
    "# Build the graph\n",
    "qualitative_chunks_retrieval_workflow.set_entry_point(\"retrieve_chunks_context_per_question\")\n",
    "\n",
    "qualitative_chunks_retrieval_workflow.add_edge(\"retrieve_chunks_context_per_question\", \"keep_only_relevant_content\")\n",
    "\n",
    "qualitative_chunks_retrieval_workflow.add_conditional_edges(\n",
    "    \"keep_only_relevant_content\",\n",
    "    is_distilled_content_grounded_on_content,\n",
    "    {\"grounded on the original context\":END,\n",
    "      \"not grounded on the original context\":\"keep_only_relevant_content\"},\n",
    "    )\n",
    "\n",
    "\n",
    "qualitative_chunks_retrieval_workflow_app = qualitative_chunks_retrieval_workflow.compile()\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        qualitative_chunks_retrieval_workflow_app.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa182e1-e181-40a4-bc74-824c986b5a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = {\"question\": \"category 3 water\"}  # The question to answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eab3cf-09ab-4046-ac1b-def78f8e8605",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test the book chunks retrieval\n",
    "for output in qualitative_chunks_retrieval_workflow_app.stream(init_state):    \n",
    "    for _, value in output.items():\n",
    "        pass \n",
    "    pprint(\"--------------------\")\n",
    "print(f'relevant context: {value[\"relevant_context\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c62ee-9492-48ff-99a0-996d406e36ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_answer_grounded_on_context(state):\n",
    "    \"\"\"Determines if the answer to the question is grounded in the facts.\n",
    "    \n",
    "    Args:\n",
    "        state: A dictionary containing the context and answer.\n",
    "    \"\"\"\n",
    "    print(\"Checking if the answer is grounded in the facts...\")\n",
    "    context = state[\"context\"]\n",
    "    answer = state[\"answer\"]\n",
    "    \n",
    "    result = is_grounded_on_facts_chain.invoke({\"context\": context, \"answer\": answer})\n",
    "    grounded_on_facts = result.grounded_on_facts\n",
    "    if not grounded_on_facts:\n",
    "        print(\"The answer is hallucination.\")\n",
    "        return \"hallucination\"\n",
    "    else:\n",
    "        print(\"The answer is grounded in the facts.\")\n",
    "        return \"grounded on context\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603f63cc-bf94-46d4-b30e-2df0930921b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualitativeAnswerGraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    context: str\n",
    "    answer: str\n",
    "\n",
    "qualitative_answer_workflow = StateGraph(QualitativeAnswerGraphState)\n",
    "\n",
    "# Define the nodes\n",
    "\n",
    "qualitative_answer_workflow.add_node(\"answer_question_from_context\",answer_question_from_context)\n",
    "\n",
    "# Build the graph\n",
    "qualitative_answer_workflow.set_entry_point(\"answer_question_from_context\")\n",
    "\n",
    "qualitative_answer_workflow.add_conditional_edges(\n",
    "\"answer_question_from_context\",is_answer_grounded_on_context ,{\"hallucination\":\"answer_question_from_context\", \"grounded on context\":END}\n",
    "\n",
    ")\n",
    "\n",
    "qualitative_answer_workflow_app = qualitative_answer_workflow.compile()\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        qualitative_answer_workflow_app.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8fe97a-f992-4af7-89f3-95aafb642a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"who is a specialized expert?\" # The question to answer\n",
    "context = \"a specialized expert is a cat.\" # The context to answer the question from\n",
    "init_state = {\"question\": question, \"context\": context} # The initial state\n",
    "for output in qualitative_answer_workflow_app.stream(init_state): \n",
    "    for _, value in output.items():\n",
    "        pass  # Node\n",
    "        # ... (your existing code)\n",
    "    pprint(\"--------------------\")\n",
    "print(f'answer: {value[\"answer\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eae93ae-a489-48ff-b460-cc49802a916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanExecute(TypedDict):\n",
    "    curr_state: str\n",
    "    question: str\n",
    "    anonymized_question: str\n",
    "    query_to_retrieve_or_answer: str\n",
    "    plan: List[str]\n",
    "    past_steps: List[str]\n",
    "    mapping: dict \n",
    "    curr_context: str\n",
    "    aggregated_context: str\n",
    "    tool: str\n",
    "    response: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdec39f9-25d8-4ffa-8c40-c6514cbe2233",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: List[str] = Field(\n",
    "        description=\"different steps to follow, should be in sorted order\"\n",
    "    )\n",
    "\n",
    "planner_prompt =\"\"\" For the given query {question}, come up with a simple step by step plan of how to figure out the answer. \n",
    "\n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "planner_prompt = PromptTemplate(\n",
    "    template=planner_prompt,\n",
    "      input_variables=[\"question\"], \n",
    "     )\n",
    "\n",
    "planner_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=2000)\n",
    "\n",
    "planner = planner_prompt | planner_llm.with_structured_output(Plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2ed7e3-ec06-4c4b-9d3d-47fff14d5d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "break_down_plan_prompt_template = \"\"\"You receive a plan {plan} which contains a series of steps to follow in order to answer a query. \n",
    "you need to go through the plan refine it according to this:\n",
    "1. every step has to be able to be executed by either:\n",
    "    i. retrieving relevant information from a vector store of book chunks\n",
    "    ii. answering a question from a given context.\n",
    "2. every step should contain all the information needed to execute it.\n",
    "\n",
    "output the refined plan\n",
    "\"\"\"\n",
    "\n",
    "break_down_plan_prompt = PromptTemplate(\n",
    "    template=break_down_plan_prompt_template,\n",
    "    input_variables=[\"plan\"],\n",
    ")\n",
    "\n",
    "break_down_plan_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=2000)\n",
    "\n",
    "break_down_plan_chain = break_down_plan_prompt | break_down_plan_llm.with_structured_output(Plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3679d8e-8026-4670-b998-75f0ea7435bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = {\"question\": \"what category of water needs biocide?\"} # The question to answer\n",
    "my_plan = planner.invoke(question) # Generate a plan to answer the question\n",
    "print(my_plan)\n",
    "refined_plan = break_down_plan_chain.invoke(my_plan.steps) # Refine the plan\n",
    "print(refined_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95117df3-e81a-4906-b1fb-fdd25a721bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActPossibleResults(BaseModel):\n",
    "    \"\"\"Possible results of the action.\"\"\"\n",
    "    plan: Plan = Field(description=\"Plan to follow in future.\")\n",
    "    explanation: str = Field(description=\"Explanation of the action.\")\n",
    "    \n",
    "\n",
    "act_possible_results_parser = JsonOutputParser(pydantic_object=ActPossibleResults)\n",
    "\n",
    "replanner_prompt_template =\"\"\" For the given objective, come up with a simple step by step plan of how to figure out the answer. \n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "\n",
    "assume that the answer was not found yet and you need to update the plan accordingly, so the plan should never be empty.\n",
    "\n",
    "Your objective was this:\n",
    "{question}\n",
    "\n",
    "Your original plan was this:\n",
    "{plan}\n",
    "\n",
    "You have currently done the follow steps:\n",
    "{past_steps}\n",
    "\n",
    "You already have the following context:\n",
    "{aggregated_context}\n",
    "\n",
    "Update your plan accordingly. If further steps are needed, fill out the plan with only those steps.\n",
    "Do not return previously done steps as part of the plan.\n",
    "\n",
    "the format is json so escape quotes and new lines.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "replanner_prompt = PromptTemplate(\n",
    "    template=replanner_prompt_template,\n",
    "    input_variables=[\"question\", \"plan\", \"past_steps\", \"aggregated_context\"],\n",
    "    partial_variables={\"format_instructions\": act_possible_results_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "replanner_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=2000)\n",
    "\n",
    "\n",
    "\n",
    "replanner = replanner_prompt | replanner_llm | act_possible_results_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ce0a76-99da-4a94-a08f-42fc60daa06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_handler_prompt_template = \"\"\"You are a task handler that receives a task {curr_task} and have to decide with tool to use to execute the task.\n",
    "You have the following tools at your disposal:\n",
    "Tool A: a tool that retrieves relevant information from a vector store of book chunks based on a given query.\n",
    "- use Tool A when you think the current task should search for information in the book chunks.\n",
    "Tool B: a tool that answers a question from a given context.\n",
    "- use Tool B ONLY when the current task can be answered by the aggregated context {aggregated_context}\n",
    "\n",
    "you also receive the last tool used {last_tool}\n",
    "if {last_tool} was retrieve_chunks, use other tools than Tool A.\n",
    "\n",
    "You also have the past steps {past_steps} that you can use to make decisions and understand the context of the task.\n",
    "You also have the initial user's question {question} that you can use to make decisions and understand the context of the task.\n",
    "if you decide to use Tools A or B, output the query to be used for the tool and also output the relevant tool.\n",
    "if you decide to use Tool B, output the question to be used for the tool, the context, and also that the tool to be used is Tool B.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class TaskHandlerOutput(BaseModel):\n",
    "    \"\"\"Output schema for the task handler.\"\"\"\n",
    "    query: str = Field(description=\"The query to be either retrieved from the vector store, or the question that should be answered from context.\")\n",
    "    curr_context: str = Field(description=\"The context to be based on in order to answer the query.\")\n",
    "    tool: str = Field(description=\"The tool to be used should be either retrieve_chunks or answer_from_context.\")\n",
    "\n",
    "\n",
    "task_handler_prompt = PromptTemplate(\n",
    "    template=tasks_handler_prompt_template,\n",
    "    input_variables=[\"curr_task\", \"aggregated_context\", \"last_tool\" \"past_steps\", \"question\"],\n",
    ")\n",
    "\n",
    "task_handler_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=2000)\n",
    "task_handler_chain = task_handler_prompt | task_handler_llm.with_structured_output(TaskHandlerOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca978cd4-f82b-4ef7-8a8d-e5a947f940a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnonymizeQuestion(BaseModel):\n",
    "    \"\"\"Anonymized question and mapping.\"\"\"\n",
    "    anonymized_question : str = Field(description=\"Anonymized question.\")\n",
    "    mapping: dict = Field(description=\"Mapping of original name entities to variables.\")\n",
    "    explanation: str = Field(description=\"Explanation of the action.\")\n",
    "\n",
    "anonymize_question_parser = JsonOutputParser(pydantic_object=AnonymizeQuestion)\n",
    "\n",
    "\n",
    "anonymize_question_prompt_template = \"\"\" You are a question anonymizer. The input You receive is a string containing several words that\n",
    " construct a question {question}. Your goal is to changes all name entities in the input to variables, and remember the mapping of the original name entities to the variables.\n",
    " ```example1:\n",
    "        if the input is \\\"what is category A water?\\\" the output should be \\\"what is X water?\\\" and the mapping should be {{\\\"X\\\": \\\"category A\\\"}} ```\n",
    "```example2:\n",
    "        if the input is \\\"how did the bad guy played with the alex and rony?\\\"\n",
    "          the output should be \\\"how did the X played with the Y and Z?\\\" and the mapping should be {{\\\"X\\\": \\\"bad guy\\\", \\\"Y\\\": \\\"alex\\\", \\\"Z\\\": \\\"rony\\\"}}```\n",
    " you must replace all name entities in the input with variables, and remember the mapping of the original name entities to the variables.\n",
    "  output the anonymized question and the mapping in a json format. {format_instructions}\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "anonymize_question_prompt = PromptTemplate(\n",
    "    template=anonymize_question_prompt_template,\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": anonymize_question_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "anonymize_question_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=2000)\n",
    "anonymize_question_chain = anonymize_question_prompt | anonymize_question_llm | anonymize_question_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4976b71f-cf90-43ba-8ccd-fe5d5957ee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeAnonymizePlan(BaseModel):\n",
    "    \"\"\"Possible results of the action.\"\"\"\n",
    "    plan: List = Field(description=\"Plan to follow in future. with all the variables replaced with the mapped words.\")\n",
    "\n",
    "de_anonymize_plan_prompt_template = \"\"\" you receive a list of tasks: {plan}, where some of the words are replaced with mapped variables. you also receive\n",
    "the mapping for those variables to words {mapping}. replace all the variables in the list of tasks with the mapped words. if no variables are present,\n",
    "return the original list of tasks. in any case, just output the updated list of tasks in a json format as described here, without any additional text apart from the\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "de_anonymize_plan_prompt = PromptTemplate(\n",
    "    template=de_anonymize_plan_prompt_template,\n",
    "    input_variables=[\"plan\", \"mapping\"],\n",
    ")\n",
    "\n",
    "de_anonymize_plan_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=2000)\n",
    "de_anonymize_plan_chain = de_anonymize_plan_prompt | de_anonymize_plan_llm.with_structured_output(DeAnonymizePlan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3a72f3-ccce-4aab-903c-465f8b3c7b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "state1 = {'question': \"how can we disinfect category 3 water? \\n\"}  # The question to answer\n",
    "print(f'question: {state1[\"question\"]}')\n",
    "anonymized_question_output = anonymize_question_chain.invoke(state1) # Anonymize the question\n",
    "anonymized_question = anonymized_question_output[\"anonymized_question\"] # Get the anonymized question\n",
    "mapping = anonymized_question_output[\"mapping\"] # Get the mapping of the original name entities to the variables\n",
    "print(f'anonimized_querry: {anonymized_question} \\n')\n",
    "print(f'mapping: {mapping} \\n')\n",
    "plan = planner.invoke({\"question\": anonymized_question}) # Generate a plan to answer the question\n",
    "print(text_wrap(f'plan: {plan.steps}'))\n",
    "print(\"\")\n",
    "deanonimzed_plan = de_anonymize_plan_chain.invoke({\"plan\": plan.steps, \"mapping\": mapping}) # De-anonymize the plan\n",
    "print(text_wrap(f'deanonimized_plan: {deanonimzed_plan.plan}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f6a948-6bea-4b05-abcc-36f0de89ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CanBeAnsweredAlready(BaseModel):\n",
    "    \"\"\"Possible results of the action.\"\"\"\n",
    "    can_be_answered: bool = Field(description=\"Whether the question can be fully answered or not based on the given context.\")\n",
    "\n",
    "can_be_answered_already_prompt_template = \"\"\"You receive a query: {question} and a context: {context}.\n",
    "You need to determine if the question can be fully answered relying only the given context.\n",
    "The only infomation you have and can rely on is the context you received. \n",
    "you have no prior knowledge of the question or the context.\n",
    "if you think the question can be answered based on the context, output 'true', otherwise output 'false'.\n",
    "\"\"\"\n",
    "\n",
    "can_be_answered_already_prompt = PromptTemplate(\n",
    "    template=can_be_answered_already_prompt_template,\n",
    "    input_variables=[\"question\",\"context\"],\n",
    ")\n",
    "\n",
    "can_be_answered_already_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=2000)\n",
    "can_be_answered_already_chain = can_be_answered_already_prompt | can_be_answered_already_llm.with_structured_output(CanBeAnsweredAlready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbccfecd-28b1-4017-a754-902da01378f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_task_handler_chain(state: PlanExecute):\n",
    "    \"\"\" Run the task handler chain to decide which tool to use to execute the task.\n",
    "    Args:\n",
    "       state: The current state of the plan execution.\n",
    "    Returns:\n",
    "       The updated state of the plan execution.\n",
    "    \"\"\"\n",
    "    state[\"curr_state\"] = \"task_handler\"\n",
    "    print(\"the current plan is:\")\n",
    "    print(state[\"plan\"])\n",
    "    pprint(\"--------------------\") \n",
    "\n",
    "    if 'past_steps' not in state:\n",
    "        state['past_steps'] = []\n",
    "    if not state['past_steps']:\n",
    "        state[\"past_steps\"] = []\n",
    "\n",
    "    curr_task = state[\"plan\"][0]\n",
    "    if 'aggregated_context' not in state:\n",
    "        state['aggregated_context'] = None  # or initialize it with an appropriate default value\n",
    "    if 'tool' not in state:\n",
    "        state['tool'] = None\n",
    "    \n",
    "\n",
    "\n",
    "    inputs = {\"curr_task\": curr_task,\n",
    "               \"aggregated_context\": state[\"aggregated_context\"],\n",
    "                \"last_tool\": state[\"tool\"],\n",
    "                \"past_steps\": state[\"past_steps\"],\n",
    "                \"question\": state[\"question\"]}\n",
    "    \n",
    "    output = task_handler_chain.invoke(inputs)\n",
    "  \n",
    "    state[\"past_steps\"].append(curr_task)\n",
    "    state[\"plan\"].pop(0)\n",
    "\n",
    "    if output.tool == \"retrieve_chunks\":\n",
    "        state[\"query_to_retrieve_or_answer\"] = output.query\n",
    "        state[\"tool\"]=\"retrieve_chunks\"\n",
    "\n",
    "    elif output.tool == \"answer_from_context\":\n",
    "        state[\"query_to_retrieve_or_answer\"] = output.query\n",
    "        state[\"curr_context\"] = output.curr_context\n",
    "        state[\"tool\"]=\"answer\"\n",
    "    else:\n",
    "        print(f\"Invalid tool: {state['tool']}, state: {state}\")\n",
    "        raise ValueError(\"Invalid tool was outputed. Must be either 'retrieve' or 'answer_from_context'\")\n",
    "    return state  \n",
    "\n",
    "def retrieve_or_answer(state: PlanExecute):\n",
    "    \"\"\"Decide whether to retrieve or answer the question based on the current state.\n",
    "    Args:\n",
    "        state: The current state of the plan execution.\n",
    "    Returns:\n",
    "        updates the tool to use .\n",
    "    \"\"\"\n",
    "    state[\"curr_state\"] = \"decide_tool\"\n",
    "    print(\"deciding whether to retrieve or answer\")\n",
    "    print(f'Current tool: {state[\"tool\"]}')\n",
    "    if state[\"tool\"] == \"retrieve_chunks\":\n",
    "        return \"chosen_tool_is_retrieve_chunks\"\n",
    "    elif state[\"tool\"] == \"answer\":\n",
    "        return \"chosen_tool_is_answer\"\n",
    "    else:\n",
    "        print(f\"Invalid tool: {state['tool']}, state: {state}\")\n",
    "        raise ValueError(\"Invalid tool was outputed. Must be either 'retrieve' or 'answer_from_context'\")  \n",
    "\n",
    "def run_qualitative_chunks_retrieval_workflow(state):\n",
    "    \"\"\"\n",
    "    Run the qualitative chunks retrieval workflow.\n",
    "    Args:\n",
    "        state: The current state of the plan execution.\n",
    "    Returns:\n",
    "        The state with the updated aggregated context.\n",
    "    \"\"\"\n",
    "    state[\"curr_state\"] = \"retrieve_chunks\"\n",
    "    print(\"Running the qualitative chunks retrieval workflow...\")\n",
    "    question = state[\"query_to_retrieve_or_answer\"]\n",
    "    inputs = {\"question\": question}\n",
    "    for output in qualitative_chunks_retrieval_workflow_app.stream(inputs):\n",
    "        for _, _ in output.items():\n",
    "            pass \n",
    "        pprint(\"--------------------\")\n",
    "    if not state[\"aggregated_context\"]:\n",
    "        state[\"aggregated_context\"] = \"\"\n",
    "    state[\"aggregated_context\"] += output['relevant_context']\n",
    "    return state\n",
    "   \n",
    "def run_qualtative_answer_workflow(state):\n",
    "    \"\"\"\n",
    "    Run the qualitative answer workflow.\n",
    "    Args:\n",
    "        state: The current state of the plan execution.\n",
    "    Returns:\n",
    "        The state with the updated aggregated context.\n",
    "    \"\"\"\n",
    "    state[\"curr_state\"] = \"answer\"\n",
    "    print(\"Running the qualitative answer workflow...\")\n",
    "    question = state[\"query_to_retrieve_or_answer\"]\n",
    "    context = state[\"curr_context\"]\n",
    "    inputs = {\"question\": question, \"context\": context}\n",
    "    for output in qualitative_answer_workflow_app.stream(inputs):\n",
    "        for _, _ in output.items():\n",
    "            pass \n",
    "        pprint(\"--------------------\")\n",
    "    if not state[\"aggregated_context\"]:\n",
    "        state[\"aggregated_context\"] = \"\"\n",
    "    state[\"aggregated_context\"] += output[\"answer\"]\n",
    "    return state\n",
    "\n",
    "def run_qualtative_answer_workflow_for_final_answer(state):\n",
    "    \"\"\"\n",
    "    Run the qualitative answer workflow for the final answer.\n",
    "    Args:\n",
    "        state: The current state of the plan execution.\n",
    "    Returns:\n",
    "        The state with the updated response.\n",
    "    \"\"\"\n",
    "    state[\"curr_state\"] = \"get_final_answer\"\n",
    "    print(\"Running the qualitative answer workflow for final answer...\")\n",
    "    question = state[\"question\"]\n",
    "    context = state[\"aggregated_context\"]\n",
    "    inputs = {\"question\": question, \"context\": context}\n",
    "    for output in qualitative_answer_workflow_app.stream(inputs):\n",
    "        for _, value in output.items():\n",
    "            pass  \n",
    "        pprint(\"--------------------\")\n",
    "    state[\"response\"] = value\n",
    "    return state\n",
    "\n",
    "def anonymize_queries(state: PlanExecute):\n",
    "    \"\"\"\n",
    "    Anonymizes the question.\n",
    "    Args:\n",
    "        state: The current state of the plan execution.\n",
    "    Returns:\n",
    "        The updated state with the anonymized question and mapping.\n",
    "    \"\"\"\n",
    "    state[\"curr_state\"] = \"anonymize_question\"\n",
    "    print(\"Anonymizing question\")\n",
    "    pprint(\"--------------------\")\n",
    "    anonymized_question_output = anonymize_question_chain.invoke(state['question'])\n",
    "    anonymized_question = anonymized_question_output[\"anonymized_question\"]\n",
    "    print(f'anonimized_querry: {anonymized_question}')\n",
    "    pprint(\"--------------------\")\n",
    "    mapping = anonymized_question_output[\"mapping\"]\n",
    "    state[\"anonymized_question\"] = anonymized_question\n",
    "    state[\"mapping\"] = mapping\n",
    "    return state\n",
    "\n",
    "\n",
    "def deanonymize_queries(state: PlanExecute):\n",
    "    \"\"\"\n",
    "    De-anonymizes the plan.\n",
    "    Args:\n",
    "        state: The current state of the plan execution.\n",
    "    Returns:\n",
    "        The updated state with the de-anonymized plan.\n",
    "    \"\"\"\n",
    "    state[\"curr_state\"] = \"de_anonymize_plan\"\n",
    "    print(\"De-anonymizing plan\")\n",
    "    pprint(\"--------------------\")\n",
    "    deanonimzed_plan = de_anonymize_plan_chain.invoke({\"plan\": state[\"plan\"], \"mapping\": state[\"mapping\"]})\n",
    "    state[\"plan\"] = deanonimzed_plan.plan\n",
    "    print(f'de-anonimized_plan: {deanonimzed_plan.plan}')\n",
    "    return state\n",
    "\n",
    "\n",
    "def plan_step(state: PlanExecute):\n",
    "    \"\"\"\n",
    "    Plans the next step.\n",
    "    Args:\n",
    "        state: The current state of the plan execution.\n",
    "    Returns:\n",
    "        The updated state with the plan.\n",
    "    \"\"\"\n",
    "    state[\"curr_state\"] = \"planner\"\n",
    "    print(\"Planning step\")\n",
    "    pprint(\"--------------------\")\n",
    "    plan = planner.invoke({\"question\": state['anonymized_question']})\n",
    "    state[\"plan\"] = plan.steps\n",
    "    print(f'plan: {state[\"plan\"]}')\n",
    "    return state\n",
    "\n",
    "\n",
    "def break_down_plan_step(state: PlanExecute):\n",
    "    \"\"\"\n",
    "    Breaks down the plan steps into retrievable or answerable tasks.\n",
    "    Args:\n",
    "        state: The current state of the plan execution.\n",
    "    Returns:\n",
    "        The updated state with the refined plan.\n",
    "    \"\"\"\n",
    "    state[\"curr_state\"] = \"break_down_plan\"\n",
    "    print(\"Breaking down plan steps into retrievable or answerable tasks\")\n",
    "    pprint(\"--------------------\")\n",
    "    refined_plan = break_down_plan_chain.invoke(state[\"plan\"])\n",
    "    state[\"plan\"] = refined_plan.steps\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "def replan_step(state: PlanExecute):\n",
    "    \"\"\"\n",
    "    Replans the next step.\n",
    "    Args:\n",
    "        state: The current state of the plan execution.\n",
    "    Returns:\n",
    "        The updated state with the plan.\n",
    "    \"\"\"\n",
    "    state[\"curr_state\"] = \"replan\"\n",
    "    print(\"Replanning step\")\n",
    "    pprint(\"--------------------\")\n",
    "    inputs = {\"question\": state[\"question\"], \"plan\": state[\"plan\"], \"past_steps\": state[\"past_steps\"], \"aggregated_context\": state[\"aggregated_context\"]}\n",
    "    output = replanner.invoke(inputs)\n",
    "    state[\"plan\"] = output['plan']['steps']\n",
    "    return state\n",
    "\n",
    "\n",
    "def can_be_answered(state: PlanExecute):\n",
    "    \"\"\"\n",
    "    Determines if the question can be answered.\n",
    "    Args:\n",
    "        state: The current state of the plan execution.\n",
    "    Returns:\n",
    "        whether the original question can be answered or not.\n",
    "    \"\"\"\n",
    "    state[\"curr_state\"] = \"can_be_answered_already\"\n",
    "    print(\"Checking if the ORIGINAL QUESTION can be answered already\")\n",
    "    pprint(\"--------------------\")\n",
    "    question = state[\"question\"]\n",
    "    context = state[\"aggregated_context\"]\n",
    "    inputs = {\"question\": question, \"context\": context}\n",
    "    output = can_be_answered_already_chain.invoke(inputs)\n",
    "    if output.can_be_answered == True:\n",
    "        print(\"The ORIGINAL QUESTION can be fully answered already.\")\n",
    "        pprint(\"--------------------\")\n",
    "        print(\"the aggregated context is:\")\n",
    "        print(text_wrap(state[\"aggregated_context\"]))\n",
    "        print(\"--------------------\")\n",
    "        return \"can_be_answered_already\"\n",
    "    else:\n",
    "        print(\"The ORIGINAL QUESTION cannot be fully answered yet.\")\n",
    "        pprint(\"--------------------\")\n",
    "        return \"cannot_be_answered_yet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3241b685-073a-47cf-bd89-4faea3ea73d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "\n",
    "agent_workflow = StateGraph(PlanExecute)\n",
    "\n",
    "# Add the anonymize node\n",
    "agent_workflow.add_node(\"anonymize_question\", anonymize_queries)\n",
    "\n",
    "# Add the plan node\n",
    "agent_workflow.add_node(\"planner\", plan_step)\n",
    "\n",
    "# Add the break down plan node\n",
    "\n",
    "agent_workflow.add_node(\"break_down_plan\", break_down_plan_step)\n",
    "\n",
    "# Add the deanonymize node\n",
    "agent_workflow.add_node(\"de_anonymize_plan\", deanonymize_queries)\n",
    "\n",
    "# Add the qualitative chunks retrieval node\n",
    "agent_workflow.add_node(\"retrieve_chunks\", run_qualitative_chunks_retrieval_workflow)\n",
    "\n",
    "# Add the qualitative answer node\n",
    "agent_workflow.add_node(\"answer\", run_qualtative_answer_workflow)\n",
    "\n",
    "# Add the task handler node\n",
    "agent_workflow.add_node(\"task_handler\", run_task_handler_chain)\n",
    "\n",
    "# Add a replan node\n",
    "agent_workflow.add_node(\"replan\", replan_step)\n",
    "\n",
    "# Add answer from context node\n",
    "agent_workflow.add_node(\"get_final_answer\", run_qualtative_answer_workflow_for_final_answer)\n",
    "\n",
    "# Set the entry point\n",
    "agent_workflow.set_entry_point(\"anonymize_question\")\n",
    "\n",
    "# From anonymize we go to plan\n",
    "agent_workflow.add_edge(\"anonymize_question\", \"planner\")\n",
    "\n",
    "# From plan we go to deanonymize\n",
    "agent_workflow.add_edge(\"planner\", \"de_anonymize_plan\")\n",
    "\n",
    "# From deanonymize we go to break down plan\n",
    "\n",
    "agent_workflow.add_edge(\"de_anonymize_plan\", \"break_down_plan\")\n",
    "\n",
    "# From break_down_plan we go to task handler\n",
    "agent_workflow.add_edge(\"break_down_plan\", \"task_handler\")\n",
    "\n",
    "# From task handler we go to either retrieve or answer\n",
    "agent_workflow.add_conditional_edges(\"task_handler\", retrieve_or_answer, {\"chosen_tool_is_retrieve_chunks\": \"retrieve_chunks\", \"chosen_tool_is_answer\": \"answer\"})\n",
    "\n",
    "# After retrieving we go to replan\n",
    "agent_workflow.add_edge(\"retrieve_chunks\", \"replan\")\n",
    "\n",
    "# After answering we go to replan\n",
    "agent_workflow.add_edge(\"answer\", \"replan\")\n",
    "\n",
    "# After replanning we check if the question can be answered, if yes we go to get_final_answer, if not we go to task_handler\n",
    "agent_workflow.add_conditional_edges(\"replan\",can_be_answered, {\"can_be_answered_already\": \"get_final_answer\", \"cannot_be_answered_yet\": \"break_down_plan\"})\n",
    "\n",
    "# After getting the final answer we end\n",
    "agent_workflow.add_edge(\"get_final_answer\", END)\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = agent_workflow.compile(checkpointer=memory)\n",
    "\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8afa173-0c82-4945-b70b-179092fe3242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import uuid\n",
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # The passenger_id is used in our flight tools to\n",
    "        # fetch the user's flight information\n",
    "        \"user_id\": str(uuid.uuid4()),\n",
    "        # Checkpoints are accessed by thread_id\n",
    "        \"thread_id\": thread_id,\n",
    "        \"recursion_limit\": 45  \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f051ac71-a862-4452-8f96-a649c2320e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_plan_and_print_steps(inputs, recursion_limit=45):\n",
    "    \"\"\"\n",
    "    Execute the plan and print the steps.\n",
    "    Args:\n",
    "        inputs: The inputs to the plan.\n",
    "        recursion_limit: The recursion limit.\n",
    "    Returns:\n",
    "        The response and the final state.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    try:    \n",
    "        for plan_output in graph.stream(inputs, config=config):\n",
    "            for _, agent_state_value in plan_output.items():\n",
    "                pass\n",
    "                print(f' curr step: {agent_state_value}')\n",
    "        response = agent_state_value['response']\n",
    "    except langgraph.pregel.GraphRecursionError:\n",
    "        response = \"The answer wasn't found in the data.\"\n",
    "    final_state = agent_state_value\n",
    "    print(text_wrap(f' the final answer is: {response}'))\n",
    "    return response, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd7fd2d-96e6-46f1-9890-2a34aedd9e0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input = {\"question\": \"what are the steps involved in restoring a ceiling that has been affected by roof leaks through a burst pipe\"}\n",
    "final_answer, final_state = execute_plan_and_print_steps(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce41e5ec-fcf3-4de4-9b3e-f0e557741ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
